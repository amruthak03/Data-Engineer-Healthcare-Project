{"cells": [{"cell_type": "code", "execution_count": 1, "id": "25a3f009-04f6-4ce5-a90f-f33350f8b7b3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import input_file_name, when\n\n# Create Spark Session\nspark = SparkSession.builder.appName(\"Healthcare Claims Ingestion\").getOrCreate()\n\n# configure variables\nBUCKET_NAME = \"healthcare-project\"\nCLAIMS_BUCKET_PATH = f\"gs://{BUCKET_NAME}/landing/claims/*.csv\" #source path\nBQ_TABLE = \"project-15f498fb-28c2-4528-bc7.bronze_dataset.claims\" #destination path\nTEMP_GCS_BUCKET = f\"{BUCKET_NAME}/temp/\"\n\n# Read data from claims source\nclaims_df = spark.read.csv(CLAIMS_BUCKET_PATH, header=True)\n\n# adding new column to get the hospital source\nclaims_df = (claims_df.withColumn(\"datasource\", when(input_file_name().contains(\"hospital2\"), \"hosb\")\n                                 .when(input_file_name().contains(\"hospital1\"), \"hosa\").otherwise(\"None\")))\n\n# Remove duplicates\nclaims_df = claims_df.dropDuplicates()\n\n# write to Bigquery\n(claims_df.write\n            .format(\"bigquery\")\n            .option(\"table\", BQ_TABLE)\n            .option(\"temporaryGcsBucket\", TEMP_GCS_BUCKET)\n            .mode(\"overwrite\")\n            .save())"}, {"cell_type": "code", "execution_count": null, "id": "d1aa34fc-17b2-47f3-8ff8-6adead73454f", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}